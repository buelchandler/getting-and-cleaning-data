# getting-and-cleaning-data
Assignment for Coursera

---
title: "README"
author: "Buel Chandler"
date: "September 23, 2015"
output: md_document
---

*If you'd like to load the resulting "tidy-ed" dataset, run the following:*
```{r}
data <- read.table("https://s3.amazonaws.com/coursera-uploads/user-e6a850d2b49e0d41ef424ccd/975116/asst-3/010ffab0649411e5b99f0d22b6a7fb62.txt", header = TRUE) 
View(data)
```

# Introduction

In this codebook, we detail what was done to get from the raw data provided by the course website and detail what steps were taken to do the five parts of the assignment proper. The end-state variables used are listed (in Part 4), as well as the transformation from raw to that end-state.

The [raw data][1] provided comes from a study that did measurements and analysis from the accelerometer and gyroscope of a Samsung smartphone being used by 30 human subjects in 6 different activities (e,g, walking, sitting). 

# Part 1: Merges the training and the test sets to create one data set.
The study data is given as a zip file. When extracted, you get a directory 
"UCI HAR Dataset", which contains 4 files and 2 sub-directories ("test" and "train").

Note: we assume you're running this from a directory which has the directory "UCI HAR Dataset" with the assigned files and sub-directories within it

So the preliminaries:
```{r}
setwd("C:/Users/Buel/datasciencecoursera/Getting and Cleaning Data")
suppressMessages(library(dplyr))
```

The next bit of code is to get all the data required, then massage and merge to get our initial raw data for subsequent transformation.

Get the activity labels (e.g, WALKING, SITTING) There are 6 obs, 2 vble. 1st variable is a digit 1 to 6 (used in the train and test datasets), and 2nd is the activity string.
```{r}
act <- read.table("UCI HAR Dataset\\activity_labels.txt")
```

Get the filtered computed features (e.g., tBodyAccMag-mean(), etc) There are 561 obs, 2 vble.
```{r}
feat <- read.table("UCI HAR Dataset\\features.txt", check.names = FALSE)
```

Get feature measurements for the *Machine Learning (ML) test population. There are 2947 obs, 561 vble. The vble correspond to the 561 obs of our table "feat" above.
```{r}
xtest <- read.table("UCI HAR Dataset\\test\\X_test.txt")
```
The datapoints for xtest occured during one of six activities. We now input the data that maps what that activity was (a digit between 1 and 6) to each row of the feature measures. 2947 obs, 1 vble 
```{r}
atest <- read.table("UCI HAR Dataset\\test\\y_test.txt")
```
Each feature observation is from one of 30 human subjects. We now read in mapping data to associate each observation of "feat" with a digit between 1 and 30 that corresponds with a particular subject. 2947 obs, 1 vble
```{r}
stest <- read.table("UCI HAR Dataset\\test\\subject_test.txt")
```

So we need to do the exact same thing for the ML training data as well. Here we end up with 3 similar datasets as above, but with 7352 observations each.
```{r}
xtrain <- read.table("UCI HAR Dataset\\train\\X_train.txt")
atrain <- read.table("UCI HAR Dataset\\train\\y_train.txt")
strain <- read.table("UCI HAR Dataset\\train\\subject_train.txt")
```

So we have 6 datasets: 2 of feature observations, and 4 that map either activity or subject to those particular observations. We merge each corresponding dataset:
```{r}
subject <- rbind(stest,strain)
activity <- rbind(atest,atrain)
measures <- rbind(xtest,xtrain)
```

When first input, the columns had genereated names (e.g., V56), so first  order of business is to get our first cut column names to the features, activity, and subject tables. Note that feature names were not unique when first autogenerated within R, so we make them unique with a **make.names** call
```{r}
colnames(measures) <- t(make.names(feat$V2, unique = TRUE, allow_ = TRUE))
colnames(activity) <- "activity"
colnames(subject) <- "subject"
```

To finish part 1 of the assignment we now create the comprehensive "raw" data table
```{r}
rawdata <- cbind(subject, activity, measures)
```

# Part 2: Extracts only the measurements on the mean and standard deviation for each measurement.

Here we extract only columns that deal with a "mean" or a "std (standard deviation), plus our newly added subject and activity data
```{r}
extractdata <- rawdata %>% 
                select(subject, 
                       activity,
                       contains("mean", ignore.case = TRUE),
                       contains("std", ignore.case = TRUE))
```

# Part 3: Uses descriptive activity names to name the activities in the data set

We transmute the activity column from integer to the actual text of the activity from our earlier read of the "act" dataset. Make it a factor
```{r}
extractdata$activity = factor(extractdata$activity, labels = act$V2)
```

# Part 4: Appropriately labels the data set with descriptive variable names

Towards the end of Part 1, the 561 features columns in the raw data were named from the "features.txt" table  The R function make.names was used to do a first pass clean-up of the names to allow us to id which columns contained "mean" and "std" attributes. That first pass cleanup was done with (from above):
>  colnames(measures) <- t(make.names(feat$V2, unique = TRUE, allow_ = TRUE))

However, the column names still contained some '...', '..', '.' which were  substituted for invalid characters and such, as well as a few other anomalies. Here we clean those up.
```{r}
# 1st convert '...' which appear followed by an axis indicater to '_'
names(extractdata) <- gsub("\\.\\.\\.", "_", names(extractdata))

# 2nd convert '..' to null, as they appear at end of names in the case
names(extractdata) <- gsub("\\.\\.", "", names(extractdata))

# 3rd convert are the singleton '.'  Here we have them both embeded between characters,
# or at the very end. The former we convert to '_', the later we drop
names(extractdata) <- gsub("\\.$", "", names(extractdata)) # '.' at end
names(extractdata) <- gsub("\\.", "_", names(extractdata)) # '.' embedded

# 4th convert initial 'f' to 'freq_', 't' to 'time_'
names(extractdata) <- gsub("^f", "freq_", names(extractdata))
names(extractdata) <- gsub("^t", "time_", names(extractdata))

# 5th convert 'angle_t' to 'angle_time_'
names(extractdata) <- gsub("angle_t", "angle_time_", names(extractdata))

# 6th convert 'meanFreq' to 'mean_Freq'
names(extractdata) <- gsub("meanFreq", "mean_Freq", names(extractdata))

# 7th and last convert. tidy up those few 'Mean' to '_mean_'
names(extractdata) <- gsub("Mean", "_mean_", names(extractdata))
names(extractdata) <- gsub("__", "_", names(extractdata))
names(extractdata) <- gsub("_$", "", names(extractdata))
```

## Data Taxonomy

*Primary Key*

**subject** (int) - ranges from 1 to 30 and corresponds to one of the 30 participants in the study

*Secondary Key*

**activity** (char/factor) - one of six activities each participant was subjected to
       (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING)

For the remainder of the variables, we provide a taxonomy that can be used to decicpher meaning. Note
that all variables below are numeric (dbl).

1. There are two sensors in the smartphone measured: Accelerometer (Acc) and Gyroscope (Gyro). Each sensor provides 3-axial signals in the X, Y and Z directions

2. The Accelerometer signal was decomposed into a Body part (Body) and a Gravity part (Gravity)

3. The recorded observations were over the time domain (time), and an Fast Fourier Transform was applied post experiment to transform to the frequency (freq) domain

4. Body linear acceleration and angular velocity were derived in time to obtain "Jerk" signals in addition

5. The magnitude (Mag) of these three-dimensional (X,Y,Z) signals were calculated using the Euclidean norm

6. a variety of statistical values were computed, of which the dataset we are creating only selected two:
	* mean is arithmatic mean 
	* std is standard deviation

7. The average of the X,Y,Z signals in a signal window sample are denoted by "angle"

Now the listing of the 86 feature variables:

**Post Process FFT Frequency Domain**
```
freq_BodyAcc_mean_Freq_X
freq_BodyAcc_mean_Freq_Y
freq_BodyAcc_mean_Freq_Z
freq_BodyAcc_mean_X
freq_BodyAcc_mean_Z
freq_BodyAcc_std_X
freq_BodyAcc_std_Y
freq_BodyAcc_std_Z
freq_BodyAccJerk_mean_Freq_X
freq_BodyAccJerk_mean_Freq_Y
freq_BodyAccJerk_mean_Freq_Z
freq_BodyAccJerk_mean_X
freq_BodyAccJerk_mean_Y
freq_BodyAccJerk_mean_Z
freq_BodyAccJerk_std_X
freq_BodyAccJerk_std_Y
freq_BodyAccJerk_std_Z
freq_BodyAccMag_mean
freq_BodyAccMag_mean_Freq
freq_BodyAccMag_std
freq_BodyBodyAccJerkMag_mean
freq_BodyBodyAccJerkMag_mean_Freq
freq_BodyBodyAccJerkMag_std
freq_BodyBodyGyroJerkMag_mean
freq_BodyBodyGyroJerkMag_mean_Freq
freq_BodyBodyGyroJerkMag_std
freq_BodyBodyGyroMag_mean
freq_BodyBodyGyroMag_mean_Freq
freq_BodyBodyGyroMag_std
freq_BodyGyro_mean_Freq_X
freq_BodyGyro_mean_Freq_Y
freq_BodyGyro_mean_Freq_Z
freq_BodyGyro_mean_X
freq_BodyGyro_mean_Y
freq_BodyGyro_mean_Z
freq_BodyGyro_std_X
freq_BodyGyro_std_Y
freq_BodyGyro_std_Z
```
**Recorded data from experiment. Time Domain**
```
time_BodyAcc_mean_X
time_BodyAcc_mean_Y
time_BodyAcc_mean_Z
time_BodyAcc_std_X
time_BodyAcc_std_Y
time_BodyAcc_std_Z
time_BodyAccJerk_mean_X
time_BodyAccJerk_mean_Y
time_BodyAccJerk_mean_Z
time_BodyAccJerk_std_X
time_BodyAccJerk_std_Y
time_BodyAccJerk_std_Z
time_BodyAccJerkMag_mean
time_BodyAccJerkMag_std
time_BodyAccMag_mean
time_BodyAccMag_std
time_BodyGyro_mean_X
time_BodyGyro_mean_Y
time_BodyGyro_mean_Z
time_BodyGyro_std_X
time_BodyGyro_std_Y
time_BodyGyro_std_Z
time_BodyGyroJerk_mean_X
time_BodyGyroJerk_mean_Y
time_BodyGyroJerk_mean_Z
time_BodyGyroJerk_std_X
time_BodyGyroJerk_std_Y
time_BodyGyroJerk_std_Z
time_BodyGyroJerkMag_mean
time_BodyGyroJerkMag_std
time_BodyGyroMag_mean
time_BodyGyroMag_std
time_GravityAcc_mean_X
time_GravityAcc_mean_Y
time_GravityAcc_mean_Z
time_GravityAcc_std_X
time_GravityAcc_std_Y
time_GravityAcc_std_Z
time_GravityAccMag_mean
time_GravityAccMag_std
```
**Average of combined X,Y,Z Space Domain**
```
angle_time_BodyAcc_mean_gravity
angle_time_BodyAccJerk_mean_gravity_mean
angle_time_BodyGyro_mean_gravity_mean
angle_time_BodyGyroJerk_mean_gravity_mean
angle_X_gravity_mean
angle_Y_gravity_mean
angle_Z_gravity_mean
```

# Part 5: From the data set in step 4, creates a second, independent tidy data set with the average of each variable for each activity and each subject.
```{r}
tidydata <- extractdata %>%
    group_by(activity, subject) %>%
    summarise_each(funs(mean))

## now we write it out
write.table(tidydata, file = "tidydata.txt", row.names = FALSE)
```

[1]: http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones "Human Activity Recognition Using Smartphones Data Set"[1]
